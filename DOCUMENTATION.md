# ğŸ“˜ Documentation du Projet LLM Chat App

## ğŸŒŸ Introduction

Ce projet est une application de chat moderne et performante permettant d'interagir avec des modÃ¨les d'intelligence artificielle (LLM). Elle combine une interface utilisateur rÃ©active construite avec **Next.js** et un backend robuste en **Rust**.

L'application supporte :

- **Multi-modÃ¨les** : Llama 3.1 8B (via Groq) et GPT-5 Mini (via OpenAI).
- **Streaming** : RÃ©ponses en temps rÃ©el via Server-Sent Events (SSE).
- **Fichiers** : Upload et analyse de fichiers (PDF, Images) pour le contexte.
- **Rendu Riche** : Support du Markdown, de la coloration syntaxique et des mathÃ©matiques (LaTeX/KaTeX).
- **Historique** : Gestion complÃ¨te des sessions de chat et archivage.

---

## ğŸ— Architecture Technique

Le projet est divisÃ© en deux parties principales : le Frontend et le Backend.

### ğŸ¨ Frontend (Dossier `app/`)

- **Framework** : [Next.js 16](https://nextjs.org/) (App Router)
- **Langage** : TypeScript / React 19
- **Styles** : [Tailwind CSS v4](https://tailwindcss.com/)
- **Composants ClÃ©s** :
  - `app/page.tsx` : Interface principale du chat (gestion de l'Ã©tat, envoi des messages, affichage).
  - `components/MarkdownRenderer.tsx` : Rendu des rÃ©ponses IA avec support MathJax/KaTeX et coloration syntaxique.

### âš™ï¸ Backend (Dossier `backend/`)

- **Langage** : [Rust](https://www.rust-lang.org/)
- **Framework Web** : [Axum](https://github.com/tokio-rs/axum)
- **Base de DonnÃ©es** : PostgreSQL (via [SQLx](https://github.com/launchbadge/sqlx))
- **FonctionnalitÃ©s** :
  - API REST pour la gestion des chats.
  - Streaming SSE pour les rÃ©ponses IA.
  - Extraction de texte depuis les PDF (`pdf-extract`).
  - Gestion des uploads de fichiers.

---

## ğŸš€ Installation et DÃ©marrage

### PrÃ©requis

- **Node.js** (v18+ recommandÃ©)
- **Rust** (Cargo)
- **PostgreSQL** (Serveur de base de donnÃ©es)

### 1. Configuration de la Base de DonnÃ©es

Assurez-vous d'avoir une base de donnÃ©es PostgreSQL active.
CrÃ©ez un fichier `.env` dans le dossier `backend/` avec les variables suivantes :

```env
DATABASE_URL=postgres://user:password@localhost:5432/nom_de_la_db
UPLOAD_DIR=uploads
UPLOAD_BASE_URL=http://127.0.0.1:4000/uploads
# ClÃ©s API pour les modÃ¨les
GROQ_API_KEY=votre_cle_groq
OPENAI_API_KEY=votre_cle_openai
```

### 2. Installation des DÃ©pendances

Ã€ la racine du projet :

```bash
npm install
```

### 3. Lancement du Projet

Le projet utilise `concurrently` pour lancer le frontend et le backend en mÃªme temps avec une seule commande :

```bash
npm run dev
```

- **Frontend** : Accessible sur [http://localhost:3000](http://localhost:3000)
- **Backend** : Accessible sur [http://127.0.0.1:4000](http://127.0.0.1:4000)

---

## ğŸ“‚ Structure du Projet

```
.
â”œâ”€â”€ app/                 # Code source du Frontend (Next.js App Router)
â”‚   â”œâ”€â”€ page.tsx         # Page principale (Chat UI)
â”‚   â”œâ”€â”€ layout.tsx       # Layout global
â”‚   â””â”€â”€ globals.css      # Styles globaux
â”œâ”€â”€ backend/             # Code source du Backend (Rust)
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â””â”€â”€ main.rs      # Point d'entrÃ©e et logique API
â”‚   â”œâ”€â”€ Cargo.toml       # DÃ©pendances Rust
â”‚   â””â”€â”€ uploads/         # Dossier de stockage des fichiers uploadÃ©s
â”œâ”€â”€ components/          # Composants React rÃ©utilisables
â”œâ”€â”€ public/              # Fichiers statiques
â””â”€â”€ package.json         # Scripts et dÃ©pendances Node
```

---

## ğŸ”Œ API Backend

Le backend expose une API RESTful sur le port 4000.

### SantÃ© du service

- `GET /health` : VÃ©rifie si le backend et la base de donnÃ©es sont opÃ©rationnels.

### Sessions de Chat

- `GET /api/chat/sessions` : Liste toutes les sessions actives.
- `POST /api/chat/sessions` : CrÃ©e une nouvelle session.
- `DELETE /api/chat/sessions/:id` : Supprime une session.
- `POST /api/chat/sessions/:id/archive` : Archive une session.

### Messages

- `POST /api/chat/sessions/:id/messages` : Ajoute un message utilisateur (rÃ©ponse synchrone).
- `POST /api/chat/sessions/:id/messages/stream` : Ajoute un message et reÃ§oit la rÃ©ponse de l'IA en **streaming (SSE)**.
- `POST /api/chat/sessions/:id/regenerate` : RÃ©gÃ©nÃ¨re le dernier message de l'IA.
- `POST /api/chat/sessions/:id/regenerate/stream` : RÃ©gÃ©nÃ¨re en streaming.

### Uploads

- `POST /api/uploads` : Upload de fichiers (Multipart). Retourne l'URL et les mÃ©tadonnÃ©es du fichier.

---

## ğŸ›  DÃ©tails Techniques

### Gestion des ModÃ¨les IA

Le backend choisit le modÃ¨le en fonction de la requÃªte :

- **Llama 3.1 8B (Groq)** : ModÃ¨le par dÃ©faut pour le texte rapide.
- **GPT-5 Mini (OpenAI)** : UtilisÃ© automatiquement si des fichiers/images sont attachÃ©s au message (multimodal).

### Base de DonnÃ©es (SchÃ©ma SimplifiÃ©)

- **chat_sessions** : `id`, `title`, `created_at`, `archived`...
- **chat_messages** : `id`, `session_id`, `role` (user/assistant), `content`, `position`...
- **chat_attachments** : `id`, `message_id`, `file_name`, `url`, `storage_key`...

### SystÃ¨me de Prompt

Un `SYSTEM_PROMPT` strict est injectÃ© pour forcer l'IA Ã  rÃ©pondre en Markdown compatible, avec des rÃ¨gles spÃ©cifiques pour les mathÃ©matiques (LaTeX) et le code.
